## 实验记录-医疗预训练-2021年12月

### 20211201

#### 正在训练的模型

* 为方便路径描述，默认SHAREFS=/sharefs/wangshuo/98bda4fa79c096c65f5d26be51c7a3ee
* 所有模型都共用相同的词表，其路径在 $SHAREFS/zqh/data.mT5/data-bin/bin-chunyu/dict.txt
* 请矫瑞更新RoBERTa模型信息在以下表格中

| #    | Model        | Data    | Enc  | Dec  | Width | Para. | Status   | WS                  | Path                                      |
| ---- | ------------ | ------- | ---- | ---- | ----- | ----- | -------- | ------------------- | ----------------------------------------- |
| 1    | BART-Base    | General | 6    | 6    | 768   | 0.43B | Training | ws-fairseq          | $SHAREFS/ws/exp/bart/base/general         |
| 2    | BART-Base    | Medical | 6    | 6    | 768   | 0.43B | Training | ws-fairseq          | $SHAREFS/ws/exp/bart/base/med             |
| 3    | BART-Base    | Mixed   | 6    | 6    | 768   | 0.43B | Training | ws-fairseq          | $SHAREFS/ws/exp/bart/base/med-and-general |
| 4    | RoBERTa-Base | General | 12   | -    | 768   | 0.27B | Training | jr-pretrain-roberta | $SHAREFS/jr/roberta/general               |
| 5    | RoBERTa-Base | Medical | 12   | -    | 768   | 0.27B | Training | jr-pretrain-roberta | $SHAREFS/jr/roberta/chunyu                |
| 6    | RoBERTa-Base | Mixed   | 12   | -    | 768   | 0.27B | Training | jr-pretrain-roberta | $SHAREFS/jr/roberta/hybrid                |
#### 评测数据

* 评测数据需要使用上述词表进行预处理，请泽远更新评测原始文本数据的路径。我看之前在 $SHARE/yzy/Medical/PreprocessData有泽远之前实验的脚本，请泽远再check一下，更新一个可用的版本

| Task   | Path (Brain++) | Status         |
| ------ | ---- | -------------- |
| cMedQQ* | /sharefs/wangshuo/98bda4fa79c096c65f5d26be51c7a3ee/DATASET/medical-yzy/cMedQQ/  | Unpreprocessed |
| cMedQA* | /sharefs/wangshuo/98bda4fa79c096c65f5d26be51c7a3ee/DATASET/medical-yzy/cMedQA/ | Unpreprocessed |
| cMedQNLI* | /sharefs/wangshuo/98bda4fa79c096c65f5d26be51c7a3ee/DATASET/medical-yzy/cMedQNLI/ |  Unpreprocessed|
| cMedIC* | /sharefs/wangshuo/98bda4fa79c096c65f5d26be51c7a3ee/DATASET/medical-yzy/cMedIC/  | Unpreprocessed |
| cMedTC* | /sharefs/wangshuo/98bda4fa79c096c65f5d26be51c7a3ee/DATASET/medical-yzy/cMedTC/ | Unpreprocessed |
| cMedIR | /sharefs/wangshuo/98bda4fa79c096c65f5d26be51c7a3ee/DATASET/medical-yzy/cMedIR/ |  Unpreprocessed|
| cMedQANER | /sharefs/wangshuo/98bda4fa79c096c65f5d26be51c7a3ee/DATASET/medical-yzy/cMedQANER/ |  Unpreprocessed|
| cEHRNER | /sharefs/wangshuo/98bda4fa79c096c65f5d26be51c7a3ee/DATASET/medical-yzy/cEHRNER/ |  Unpreprocessed|
| CMDD | /sharefs/wangshuo/98bda4fa79c096c65f5d26be51c7a3ee/DATASET/medical-yzy/CMDD/ |  Unpreprocessed|

注1：标\*表示路径下包含处理成language model的数据，原始数据为train/dev/test.txt，也可以直接重新解压对应的数据压缩包

注2：CMDD任务MC-Bert的论文里没有报结果，也没有介绍，但是开源的数据集里有这么个任务

#### 评测结果

* 请至诚评测BART模型，更新结果到下面表格
* 请祥哲评测RoBERTa模型，更新结果到下面表格
* 由于模型训练比预想的时间要长，我们先统一选择15000步的checkpoint来进行评测，跑通评测流程

> BART、RoBERTa均先选了20k步的预训练checkpoint。之前二分类的F1在调sklearn的接口时错用了'micro'的计算方法，所以重新算了一下结果，影响到的任务是QQ、QA、QNLI

|  #   |    Model     |  Data   | cMedQQ (F1) | cMedQA (F1) | cMedQNLI (F1) | cMedIC (F1) | cMedTC (F1) | cMedIR (PAIR) | cMedQANER (F1) | cEBRER (F1) |
| :--: | :----------: | :-----: | :---------: | ----------- | ------------- | ----------- | ----------- | ------------- | -------------- | ----------- |
|  B1  |  Bert-Base   |    -    |    86.5     | 81.0        | 93.3          | 86.0        | 79.0        | 1.77          |                |             |
|  B2  |   MC-BERT    |    -    |  **87.5**   | 82.3        | 95.5          | 87.5        | 82.1        | 2.04          |                |             |
|      |              |         |             |             |               |             |             |               |                |             |
|  1   |  BART-Base   | General |    70.7     |             |               | 83.1        |             |               |                |             |
|  2   |  BART-Base   | Medical |    68.6     |             |               | 78.3        |             |               |                |             |
|  3   |  BART-Base   |  Mixed  |    69.7     |             |               | 67.5        |             |               |                |             |
|      |              |         |             |             |               |             |             |               |                |             |
|  4   | RoBERTa-Base | General |    84.5     | 94.7        | 94.8          | 91.6        |             | 2.53          |                |             |
|  5   | RoBERTa-Base | Medical |  **84.8**   | **96.2**    | **96.2**      | 92.8        |             | **2.63**      |                |             |
|  6   | RoBERTa-Base |  Mixed  |  **84.8**   | 95.3        | 96.0          | **94.0**    |             | 1.92          |                |             |
|      |              |         |             |             |               |             |             |               |                |             |
|  7   |  GPT2-Base   | General |             |             |               |             |             |               |                |             |
|  8   |  GPT2-Base   | Medical |    82.6     | 92.2        | 90.4          | 83.3        | 20.4        | 1.34          |                |             |
|  9   |  GPT2-Base   |  Mixed  |             |             |               |             |             |               |                |             |
